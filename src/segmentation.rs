// src/segmentation.rs
use crossbeam_channel::{
    Receiver as CrossbeamReceiver, Sender as CrossbeamSender, TryRecvError, TrySendError,
};
use egui::{ColorImage, Pos2, Rect as EguiRect};
use image::{DynamicImage, RgbImage}; // Keep RgbImage for receiving
use imageproc::point::Point;
use log::{debug, error, info, warn};
use rand::rngs::SmallRng;
use rand::{Rng, SeedableRng};
use usls::Nms;
use std::{
    collections::HashSet,
    f32::consts::PI,
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
    thread::{self, JoinHandle},
    time::{Duration, Instant},
};

use usls::{models::YOLO, Bbox, Options, Y};

// Import the message type from camera.rs
use crate::camera::CameraToSegMsg; // Use the renamed type

#[derive(Debug, Clone)]
pub enum UserInteractionSegMsg {}

#[derive(Debug, Clone)]
pub struct VisualObjectData {
    pub bbox: EguiRect,
    pub contours: Vec<Vec<Pos2>>,
    pub band_index: usize,
    pub intensity: f32,
    pub animation_phase: f32,
}

#[derive(Debug)]
pub enum SegmentationOutputMsg {
    FrameData {
        // Keep original_frame for now, simplifies UI texture handling
        original_frame: Arc<ColorImage>,
        objects: Vec<VisualObjectData>,
        processing_time: Duration,
    },
    Error(String),
}

pub const MAX_TRACKS: usize = 3;
const IOU_THRESHOLD: f32 = 0.3;

#[derive(Debug, Clone)]
struct TrackedObject {
    last_bbox_usls: Bbox,
    contours: Vec<Vec<Point<i32>>>,
    band_index: usize,
    animation_phase: f32,
}

pub fn start_segmentation_thread(
    ui_sender: CrossbeamSender<SegmentationOutputMsg>,
    // Use renamed type
    camera_receiver: CrossbeamReceiver<CameraToSegMsg>,
    _user_interaction_receiver: CrossbeamReceiver<UserInteractionSegMsg>,
    intensity_receiver: CrossbeamReceiver<Vec<f32>>,
    stop_signal: Arc<AtomicBool>,
    ctx: egui::Context,
    model_options: Options,
) -> JoinHandle<()> {
    info!("Spawning segmentation thread (epaint Viz - using Polygons)");
    thread::spawn(move || {
        segmentation_loop(
            ui_sender,
            camera_receiver,
            intensity_receiver,
            stop_signal,
            ctx,
            model_options,
        );
    })
}

// ... (helper functions usls_bbox_to_egui_rect, geo_coord_to_pos2, convert_usls_polygon_to_contours remain the same) ...
fn geo_coord_to_pos2(p: &geo::Coord) -> Pos2 {
    Pos2::new(p.x as f32, p.y as f32)
}

fn convert_usls_polygon_to_contours(polygon: &usls::Polygon) -> Vec<Vec<Pos2>> {
    let exterior_points: Vec<Pos2> = polygon
        .polygon()
        .exterior()
        .coords()
        .map(geo_coord_to_pos2)
        .collect();
    vec![exterior_points]
}
fn usls_bbox_to_egui_rect(bbox: &Bbox) -> EguiRect {
    EguiRect::from_min_max(
        Pos2::new(bbox.xmin(), bbox.ymin()),
        Pos2::new(bbox.xmax(), bbox.ymax()),
    )
}


fn segmentation_loop(
    ui_sender: CrossbeamSender<SegmentationOutputMsg>,
    // Use renamed type
    camera_receiver: CrossbeamReceiver<CameraToSegMsg>,
    intensity_receiver: CrossbeamReceiver<Vec<f32>>,
    stop_signal: Arc<AtomicBool>,
    ctx: egui::Context,
    model_options: Options,
) {
    info!("Segmentation loop started (epaint Viz - using Polygons).");

    if !model_options.find_contours() {
        warn!("Model option 'find_contours' is false. Polygons might not be generated by usls. Visualizations might be empty.");
    }

    let mut model = match YOLO::new(model_options) {
        Ok(m) => m,
        Err(e) => {
            let emsg = format!("Model load failed: {}", e);
            error!("{}", emsg);
            let _ = ui_sender.try_send(SegmentationOutputMsg::Error(emsg)); // Use try_send
            ctx.request_repaint();
            return;
        }
    };

    let mut tracked_objects: Vec<TrackedObject> = Vec::new();
    let mut processing_time = Duration::from_secs(0);
    let mut current_band_intensities = vec![0.0f32; MAX_TRACKS];
    let mut rng = SmallRng::from_rng(&mut rand::thread_rng()); // Faster RNG

    while !stop_signal.load(Ordering::Relaxed) {
        // --- Receive Frame ---
        // Use try_recv in a loop to get the *latest* frame, dropping older ones if lagging
        let mut latest_frame_arc: Option<Arc<RgbImage>> = None;
        loop {
            match camera_receiver.try_recv() {
                 // Use renamed type
                Ok(CameraToSegMsg::Frame(f)) => {
                    latest_frame_arc = Some(f); // Keep the latest
                }
                Ok(CameraToSegMsg::Error(e)) => {
                    warn!("Received camera error: {}", e);
                    // Handle error appropriately, maybe clear latest_frame_arc?
                    latest_frame_arc = None;
                }
                Err(TryRecvError::Empty) => {
                    break; // No more frames available for now
                }
                Err(TryRecvError::Disconnected) => {
                    error!("Camera disconnected.");
                    stop_signal.store(true, Ordering::Relaxed);
                    break; // Break inner loop
                }
            }
        }
         if stop_signal.load(Ordering::Relaxed) { break; } // Check stop signal more often

        // --- Receive Intensities (keep latest) ---
        loop {
            match intensity_receiver.try_recv() {
                Ok(i) => {
                     if i.len() >= MAX_TRACKS {
                        current_band_intensities.copy_from_slice(&i[0..MAX_TRACKS]);
                    } else {
                        current_band_intensities.fill(0.0);
                        if !i.is_empty() {
                            current_band_intensities[0..i.len()].copy_from_slice(&i);
                        }
                    }
                },
                Err(TryRecvError::Empty) => break,
                Err(TryRecvError::Disconnected) => {
                    error!("Audio proc disconnected.");
                    stop_signal.store(true, Ordering::Relaxed);
                    break;
                }
            }
        }
         if stop_signal.load(Ordering::Relaxed) { break; }

        // --- Process Frame ---
        if let Some(frame_arc) = latest_frame_arc { // Only process if we have a frame
            let loop_start_time = Instant::now();
            let original_image_rgb = (*frame_arc).clone(); // Still need RgbImage for conversion
            let dynamic_image = DynamicImage::ImageRgb8(original_image_rgb.clone());
            let images_slice = [dynamic_image];

            let proc_start = Instant::now();
            let results_ys = model.forward(&images_slice);
            processing_time = proc_start.elapsed();

            let mut visual_objects_for_this_frame: Vec<VisualObjectData> = Vec::new();
            let mut next_tracked_objects_state: Vec<TrackedObject> = Vec::new();

            match results_ys {
                Ok(ys) => {
                    if let Some(y) = ys.first() {
                        let current_bboxes = y.bboxes().unwrap_or_default();
                        let current_polygons = y.polygons().unwrap_or_default();

                        // --- Tracking Logic (unchanged) ---
                        let mut matched_current_indices: HashSet<usize> = HashSet::new();
                        for tracked_obj_state in tracked_objects.iter() {
                             let mut best_match_for_this_track: Option<(usize, f32)> = None;
                             for (det_idx, current_bbox) in current_bboxes.iter().enumerate() {
                                if matched_current_indices.contains(&det_idx) { continue; }
                                let iou = tracked_obj_state.last_bbox_usls.iou(current_bbox);
                                if iou > IOU_THRESHOLD {
                                    let is_better = best_match_for_this_track.map_or(true, |(_, cur_iou)| iou > cur_iou);
                                    if is_better {
                                        best_match_for_this_track = Some((det_idx, iou));
                                    }
                                }
                            }
                            if let Some((matched_det_idx, _iou)) = best_match_for_this_track {
                                if let Some(matched_polygon) = current_polygons.get(matched_det_idx) {
                                    let updated_animation_phase = tracked_obj_state.animation_phase + 0.05 + current_band_intensities[tracked_obj_state.band_index] * 0.1;
                                    visual_objects_for_this_frame.push(VisualObjectData {
                                        bbox: usls_bbox_to_egui_rect(&current_bboxes[matched_det_idx]),
                                        contours: convert_usls_polygon_to_contours(matched_polygon),
                                        band_index: tracked_obj_state.band_index,
                                        intensity: current_band_intensities[tracked_obj_state.band_index],
                                        animation_phase: updated_animation_phase,
                                    });
                                    next_tracked_objects_state.push(TrackedObject {
                                        last_bbox_usls: current_bboxes[matched_det_idx].clone(),
                                        contours: vec![], // Placeholder
                                        band_index: tracked_obj_state.band_index,
                                        animation_phase: updated_animation_phase,
                                    });
                                    matched_current_indices.insert(matched_det_idx);
                                } else {
                                     warn!("Matched bbox index {} has no corresponding polygon. Skipping track update.", matched_det_idx);
                                }
                            }
                        }
                        for det_idx in 0..current_bboxes.len().min(current_polygons.len()) {
                             if !matched_current_indices.contains(&det_idx) {
                                let new_polygon = &current_polygons[det_idx];
                                let assigned_band = rng.gen_range(0..MAX_TRACKS);
                                let initial_animation_phase = rng.gen::<f32>() * 2.0 * PI;
                                visual_objects_for_this_frame.push(VisualObjectData {
                                    bbox: usls_bbox_to_egui_rect(&current_bboxes[det_idx]),
                                    contours: convert_usls_polygon_to_contours(new_polygon),
                                    band_index: assigned_band,
                                    intensity: current_band_intensities[assigned_band],
                                    animation_phase: initial_animation_phase,
                                });
                                next_tracked_objects_state.push(TrackedObject {
                                    last_bbox_usls: current_bboxes[det_idx].clone(),
                                    contours: vec![], // Placeholder
                                    band_index: assigned_band,
                                    animation_phase: initial_animation_phase,
                                });
                            }
                        }
                        tracked_objects = next_tracked_objects_state;
                    }
                }
                Err(e) => {
                    warn!("Model forward pass failed: {}", e);
                }
            }

            // --- Convert RgbImage to ColorImage for UI ---
            let frame_for_ui = {
                let size = [
                    original_image_rgb.width() as usize,
                    original_image_rgb.height() as usize,
                ];
                 // Consumes original_image_rgb here
                let pixels = original_image_rgb.into_raw();
                ColorImage::from_rgb(size, &pixels)
            };

            // --- Send Data to UI (non-blocking) ---
            let output_msg = SegmentationOutputMsg::FrameData {
                original_frame: Arc::new(frame_for_ui),
                objects: visual_objects_for_this_frame,
                processing_time,
            };

            match ui_sender.try_send(output_msg) {
                Ok(_) => {
                    // Request repaint indirectly via camera thread
                }
                Err(TrySendError::Full(_)) => {
                     warn!("UI channel full. Dropping segmentation results.");
                }
                Err(TrySendError::Disconnected(_)) => {
                    info!("UI disconnected.");
                    stop_signal.store(true, Ordering::Relaxed);
                    break; // Break outer loop
                }
            }

             debug!(
                "Seg loop: {:.2?}, Model: {:.2?}, Tracked: {}, AudioInt: [{:.2}, {:.2}, {:.2}]",
                loop_start_time.elapsed(),
                processing_time,
                tracked_objects.len(),
                current_band_intensities.get(0).cloned().unwrap_or(0.0),
                current_band_intensities.get(1).cloned().unwrap_or(0.0),
                current_band_intensities.get(2).cloned().unwrap_or(0.0)
            );

        } else {
            // No frame received from camera channel, sleep briefly
            thread::sleep(Duration::from_millis(5));
        }
         if stop_signal.load(Ordering::Relaxed) { break; }
    }

    info!("Segmentation loop finishing.");
}